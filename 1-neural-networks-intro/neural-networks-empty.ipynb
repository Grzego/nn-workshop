{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane treningowe\n",
    "\n",
    "Ponieważ będziemy potrzebowali na czymś wytrenować naszą sieć neuronową skorzystamy z popularnego zbioru w Machine Learningu czyli MNIST. Zbiór ten zawiera ręcznie pisane cyfry od 0 do 9. Są to niewielkie obrazki o wielkości 28x28 pixeli.\n",
    "\n",
    "Pobierzmy i załadujmy zbiór."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skorzystamy z gotowej funkcji do pobrania tego zbioru\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimportujmy dodatkowe biblioteki do wyświetlania wykresów/obrazów oraz `numpy` który jest paczką do obliczeń na macierzach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # można osobno doinstalować tą paczke (rysuje ładniejsze wykresy)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# pozwala na rysowanie w notebooku (nie otwiera osobnego okna)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy ile jest przykładów w zbiorze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mnist.data.shape)  # 28x28\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wyświetlmy pare przykładowych obrazków ze zbioru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    r = np.random.randint(0, len(mnist.data))\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.title(mnist.target[r])\n",
    "    plt.imshow(mnist.data[r].reshape((28, 28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie sieci neuronowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby za bardzo nie komplikować sprawy stworzymy sieć o trzech warstwach. Na początku ustalmy ilość neuronów w każdej z warstw. Ponieważ wielkość obrazka to 28x28 pixeli potrzebujemy więc 784 neuronów wejściowych. W warstwie ukrytej możemy ustawić ilość na dowolną. Ponieważ mamy do wyboru 10 różnych cyfr tyle samo neuronów damy w warstwie wyjściowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = 784\n",
    "hidden_layer = ...\n",
    "output_layer = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kluczowym elementem sieci neuronowych są ich wagi na połączeniach między neuronami. Aktualnie po prostu wczytamy już wytrenowane wagi dla sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wcztanie już wytrenowanych wag (parametrów)\n",
    "import h5py\n",
    "with h5py.File('weights.h5', 'r') as file:\n",
    "    W1 = file['W1'][:]\n",
    "    W2 = file['W2'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczenia wykonywane przez sieć neuronową można rozrysować w postaci grafu obliczeniowego, gdzie każdy z wierzchołków reprezentuje jakąś operację na wejściach. Wykorzystywana przez nas sieć przedstawiona jest na grafie poniżej (`@` to mnożenie macierzy):\n",
    "\n",
    "![](img/nn-forward-graph.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, w1, w2):\n",
    "    # x - wejście sieci\n",
    "    # w1 - parametry warstwy ukrytej\n",
    "    # w2 - parametry warstwy wyjściowej\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uruchomienie sieci i sprawdzenie jej działania\n",
    "# użyj funkcji forward_pass dla kilku przykładów i zobacz czy sieć odpowiada poprawnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie sieci (Back-propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy przygotować dane pod trenowanie sieci. Chodzi tu głównie o zakodowanie `mnist.target` w sposób 'one-hot encoding'. Czyli: $$y = \\left[ \\begin{matrix} 0 \\\\ 1 \\\\ 2 \\\\ \\vdots \\\\ 8 \\\\ 9 \\end{matrix} \\right] \\Longrightarrow \\left[ \\begin{matrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\end{matrix} \\right]$$\n",
    "\n",
    "**Uwaga:** aktualnie wszystkie dane są posortowane względem odpowiedzi. Czyli wszystkie zera są na początku póżniej są jednyki, itd. Takie ustawienie może w znaczący sposób utrudnić trenowanie sieci. Dlatego należy dane na starcie \"przetasować\". Trzeba przy tym pamiętać, żeby wejścia dalej odpowiadały tym samym wyjściom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = ...\n",
    "y_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na starcie parametry są zwyczajnie losowane. Wykorzystamy do tego funkcje `np.random.rand(dim_1, dim_2, ..., dim_n)` losuje ona liczby z przedziału $[0, 1)$ i zwraca tensor o podanych przez nas wymiarach.\n",
    "\n",
    "**Uwaga:** Mimo, że funkcja zwraca liczby z przedziału $[0, 1)$ nasze startowe parametry powinny być z przedziału $(-0.01, 0.01)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = ...\n",
    "W2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacja propagacji wstecznej\n",
    "\n",
    "Podobnie jak przy optymalizowaniu funkcji, do wyliczenia gradientów wykorzystamy _backprop_. Graf obliczeniowy jest trochę bardziej skomplikowany. (`@` oznacza mnożenie macierzy)\n",
    "\n",
    "![Backward graph](img/nn-backward-graph.png \"Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do zaimplementowania funkcji `back_prop(...)` będziemy jeszcze potrzebować pochodnych dla naszych funkcji oraz funkcje straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    # y_true - poprawna odpowiedź\n",
    "    # y_pred - odpowiedź wyliczona przez sieć neuronową\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # implementacja\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_derivative(y_true, y_pred):\n",
    "    # y_true - poprawna odpowiedź\n",
    "    # y_pred - odpowiedź wyliczona przez sieć neuronową\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(x, y, w1, w2):\n",
    "    # x - wejście sieci\n",
    "    # y - poprawne odpowiedzi\n",
    "    # w1 - parametry warstwy ukrytej\n",
    "    # w2 - parametry warstwy wyjściowej\n",
    "    \n",
    "    # zastąp linie pod spodem kodem z funkcji forward_pass\n",
    "    # >>>\n",
    "    ...\n",
    "    # <<<\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return loss, dw1, dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napiszemy jeszcze funkcje, która będzie wykonywała jeden krok optymalizacji dla podanych parametrów i ich gradientów o podanym kroku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_gradients(w1, w2, dw1, dw2, learning_rate):\n",
    "    # w1 - parametry warstwy ukrytej\n",
    "    # w2 - parametry warstwy wyjściowej\n",
    "    # dw1 - gradienty dla parametrów warstwy ukrytej\n",
    "    # dw2 - gradienty dla parametrów warstwy wyjściowej\n",
    "    # learning_rate - krok optymalizacji\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby móc lepiej ocenić postęp uczenia się sieci napiszemy funkcje, która będzie wyliczać jaki procent odpowiedzi udzielanych przez sieć neuronową jest poprawny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(x, y, w1, w2):\n",
    "    # x - wejście sieci\n",
    "    # y - poprawne odpowiedzi\n",
    "    # w1 - parametry warstwy ukrytej\n",
    "    # w2 - parametry warstwy wyjściowej\n",
    "    \n",
    "    # hint: użyj funkcji forward_pass i np.argmax\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W końcu możemy przejść do napisania głównej pętli uczącej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 5  # ile razy będziemy iterować po danych treningowych\n",
    "learning_rate = 0.001\n",
    "batch_size = 16  # na jak wielu przykładach na raz będziemy trenować sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(nb_epoch):\n",
    "    print('\\nEpoch %d' % (epoch,))\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = ...\n",
    "        y_batch = ...\n",
    "\n",
    "        # wykonaj back_prop dla pojedynczego batch'a\n",
    "        ...\n",
    "        \n",
    "        # zaktualizuj parametry\n",
    "        ...\n",
    "        \n",
    "        losses.append(loss)\n",
    "        print('\\r[%5d/%5d] loss: %8.6f - accuracy: %10.6f' % (i + 1, len(x_train),\n",
    "                  loss, accuracy(x_batch, y_batch, W1, W2)), end='')\n",
    "        \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Dokładność dla całego zbioru:', accuracy(x_train, y_train, W1, W2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
