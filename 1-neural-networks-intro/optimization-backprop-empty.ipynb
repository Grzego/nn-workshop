{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizacja i propagacja wsteczna (backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczniemy od prostego przykładu. Funkcji kwadratowej: $f(x) = x^2$\n",
    "\n",
    "Spróbujemy ją zoptymalizować (znaleźć minimum) przy pomocy metody zwanej _gradient descent_. Polega ona na wykorzystaniu gradientu (wartości pierwszej pochodnej) przy wykonywaniu kroku optymalizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, x**2, label='f(x)')  # optymalizowana funkcja\n",
    "plt.plot(x, 2 * x, label='pochodna -- f\\'(x)')  # pochodna\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja ta ma swoje minimum w punkcie $x = 0$. Jak widać na powyższym rysunku, gdy pochodna jest dodatnia (co oznacza, że funkcja jest rosnąca) lub gdy pochodna jest ujemna (gdy funkcja jest malejąca), żeby zminimalizować wartość funkcji potrzebujemy wykonywać krok optymalizacji w kierunku **przeciwnym** do tego wyznaczanego przez gradient.\n",
    "\n",
    "Przykładowo gdybyśmy byli w punkcie $x = 2$, gradient wynosiłby $4$. Ponieważ jest dodatni, żeby zbliżyć się do minimum potrzebujemy przesunąć naszą pozycje w kierunku przeciwnym czyli w stonę ujemną.\n",
    "\n",
    "Ponieważ gradient nie mówi nam dokładnie jaki krok powinniśmy wykonać, żeby dotrzeć do minimum a raczej wskazuje kierunek. Żeby nie \"przeskoczyć\" minimum zwykle skaluje się krok o pewną wartość $\\alpha$ nazywaną _krokiem uczenia_ (ang. learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prosty przykład optymalizacji $f(x) = x^2$ przy użyciu gradient descent.\n",
    "\n",
    "Sprawdź różne wartości `learning_step`, w szczególności `[0.1, 1.0, 1.1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = ...\n",
    "nb_steps = 10\n",
    "x_ = 1\n",
    "\n",
    "steps = [x_]\n",
    "for _ in range(nb_steps):\n",
    "    x_ -= learning_rate * (2 * x_)  # learning_rate * pochodna\n",
    "    steps += [x_]\n",
    "    \n",
    "plt.plot(x, x**2, alpha=0.7)\n",
    "plt.plot(steps, np.array(steps)**2, 'r-', alpha=0.7)\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-1, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop - propagacja wsteczna - metoda liczenia gradientów przy pomocy _reguły łańcuchowej_ (ang. chain rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozpatrzymy przykład minimalizacji troche bardziej skomplikowanej jednowymiarowej funkcji\n",
    "$$f(x) = \\frac{x \\cdot \\sigma(x)}{x^2 + 1}$$\n",
    "\n",
    "Do optymalizacji potrzebujemy gradientu funkcji. Do jego wyliczenia skorzystamy z _chain rule_ oraz grafu obliczeniowego.\n",
    "\n",
    "Chain rule mówi, że:\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby łatwiej zastosować _chain rule_ stworzymy z funkcji _graf obliczeniowy_, którego wykonanie zwróci nam wynik funkcji.\n",
    "\n",
    "![Graf obliczeniowy](img/comp-graph.png \"Graf obliczeniowy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backprop w grafie](img/backprop-comp-graph.png \"Graf obliczeniowy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli do węzła przychodzi więcej niż jedna krawędź (np. węzeł _x_), gradienty sumujemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x):\n",
    "    # kopia z forward_pass, ponieważ potrzebujemy wartości\n",
    "    # pośrednich by wyliczyć pochodne\n",
    "    # >>>\n",
    "    ...\n",
    "    # <<<\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "plt.plot(x, forward_pass(x), label='f(x)')\n",
    "plt.plot(x, backward_pass(x), label='poochodna -- f\\'(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posiadając gradient możemy próbować optymalizować funkcję, podobnie jak poprzenio.\n",
    "\n",
    "Sprawdź różne wartości parametru `x_`, który oznacza punkt startowy optymalizacji. W szczególności zwróć uwagę na wartości `[-5.0, 1.3306, 1.3307, 1.330696146306314]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "nb_steps = 100\n",
    "x_ = ...\n",
    "\n",
    "steps = [x_]\n",
    "for _ in range(nb_steps):\n",
    "    x_ -= learning_rate * backward_pass(x_)\n",
    "    steps += [x_]\n",
    "\n",
    "plt.plot(x, forward_pass(x), alpha=0.7)\n",
    "plt.plot(steps, forward_pass(np.array(steps)), 'r-', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jakie pojawiają się problemy?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
